# -*- coding: utf-8 -*-
"""IR-HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HMk1_Nupzz56tPyY-m_QCiin3qcLbhIt

# Imports
"""

import pandas as pd
import csv

"""# Read File"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_excel('/content/final_books.xlsx')
row = [2]
df = df.get('content')

"""# install hazm"""

!pip install hazm

"""# Normalization"""

import hazm

informalNormalizer = hazm.InformalNormalizer

normalizer = hazm.Normalizer()

df = df.apply(lambda x: normalizer.character_refinement(normalizer.affix_spacing(str(x))))
df.head()

df = df.apply(lambda x: normalizer.normalize(str(x)))
df.head()

!git clone https://github.com/kharazi/persian-stopwords.git

word_tokenize = hazm.word_tokenize
token_df = pd.DataFrame()
token_df['tokens'] =df.apply(lambda x: word_tokenize(x))
token_df

chars = []
with open('/content/persian-stopwords/chars', 'r') as f:
    for line in f:
      chars.append(line.strip())

def remove_stopwords(tokens):
  sw = hazm.utils.stopwords_list()
  return [item for item in tokens if item not in chars and item not in sw]

token_df['without-stop-word'] = token_df['tokens'].apply(lambda x: remove_stopwords(x))
token_df['without-stop-word']

"""# lemmatizer"""

def stem(tokens):
  stemmer = hazm.Stemmer()
  return [stemmer.stem(item) for item in tokens]

def lem(tokens):
  lemmatizer = hazm.Lemmatizer()
  return [lemmatizer.lemmatize(x) for x in tokens]

token_df['stemmed'] = token_df['without-stop-word'].apply(lambda x: stem(x))
token_df['stemmed'].head()

token_df['lemmetized'] = token_df['stemmed'].apply(lambda x: lem(x))
token_df['lemmetized'].head()

for row in token_df['without-stop-word']:
  print(lem(row))

token_df['lem'] = lem(item for item in token_df['without-stop-word'])

"""# Inverted Index 
after lemmatization

"""

docs = token_df["lemmetized"].tolist()
index_map = {}
for doc in docs:
  doc_index = docs.index(doc) + 1
  doc = list(set(doc))
  for token in doc:
    if token not in index_map.keys():
      index_map[token] = [doc_index,]
    else: 
      index_map[token].append(doc_index)

"""write output of inverted index to a csv file"""

with open("/content/II_lem_output.csv", 'w', newline='') as output_file:
  csvwriter = csv.writer(output_file)
  for key in index_map.keys():
    row = []
    row.append(key)
    for i in index_map[key]:
      row.append(i)
    csvwriter.writerow(row)

"""# Inverted Index
after stemming
"""

docs = token_df["stemmed"].tolist()
index_map = {}
for doc in docs:
  doc_index = docs.index(doc) + 1
  doc = list(set(doc))
  for token in doc:
    if token not in index_map.keys():
      index_map[token] = [doc_index,]
    else: 
      index_map[token].append(doc_index)

"""write output of inverted index to a csv file"""

with open("/content/II_stm_output.csv", 'w', newline='') as output_file:
  csvwriter = csv.writer(output_file)
  for key in index_map.keys():
    row = []
    row.append(key)
    for i in index_map[key]:
      row.append(i)
    csvwriter.writerow(row)